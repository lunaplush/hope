{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LQ7i1HkmYY68"
   },
   "source": [
    "<p style=\"align: center;\"><img align=center src=\"https://drive.google.com/uc?export=view&id=1TRNaCfYstvcIQqoUSdukYQGF6LuyL7Tv\" width=600 height=320/></p>\n",
    "<h3 style=\"text-align: center;\"><b>Школа глубокого обучения ФПМИ МФТИ</b></h3>\n",
    "\n",
    "# Домашнее задание. Автоэнкодеры\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wru2LNFuL2Iq"
   },
   "source": [
    "# Часть 1. Vanilla Autoencoder (10 баллов)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kr3STtdpYY7G"
   },
   "source": [
    "## 1.1. Подготовка данных (0.5 балла)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "xTNi9JLRYY7I"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from torch.autograd import Variable\n",
    "from torchvision import datasets\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data_utils\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import skimage.io\n",
    "from skimage.transform import resize\n",
    "\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "zvAjov5F2NvE"
   },
   "outputs": [],
   "source": [
    "def fetch_dataset(attrs_name = \"lfw_attributes.txt\",\n",
    "                      images_name = \"lfw-deepfunneled\",\n",
    "                      dx=80,dy=80,\n",
    "                      dimx=64,dimy=64\n",
    "    ):\n",
    "\n",
    "    #download if not exists\n",
    "    if not os.path.exists(images_name):\n",
    "        print(\"images not found, donwloading...\")\n",
    "        os.system(\"wget http://vis-www.cs.umass.edu/lfw/lfw-deepfunneled.tgz -O tmp.tgz\")\n",
    "        print(\"extracting...\")\n",
    "        os.system(\"tar xvzf tmp.tgz && rm tmp.tgz\")\n",
    "        print(\"done\")\n",
    "        assert os.path.exists(images_name)\n",
    "\n",
    "    if not os.path.exists(attrs_name):\n",
    "        print(\"attributes not found, downloading...\")\n",
    "        os.system(\"wget http://www.cs.columbia.edu/CAVE/databases/pubfig/download/%s\" % attrs_name)\n",
    "        print(\"done\")\n",
    "\n",
    "    #read attrs\n",
    "    df_attrs = pd.read_csv(\"lfw_attributes.txt\",sep='\\t',skiprows=1,) \n",
    "    df_attrs = pd.DataFrame(df_attrs.iloc[:,:-1].values, columns = df_attrs.columns[1:])\n",
    "\n",
    "\n",
    "    #read photos\n",
    "    photo_ids = []\n",
    "    for dirpath, dirnames, filenames in os.walk(images_name):\n",
    "        for fname in filenames:\n",
    "            if fname.endswith(\".jpg\"):\n",
    "                fpath = os.path.join(dirpath,fname)\n",
    "                photo_id = fname[:-4].replace('_',' ').split()\n",
    "                person_id = ' '.join(photo_id[:-1])\n",
    "                photo_number = int(photo_id[-1])\n",
    "                photo_ids.append({'person':person_id,'imagenum':photo_number,'photo_path':fpath})\n",
    "\n",
    "    photo_ids = pd.DataFrame(photo_ids)\n",
    "    # print(photo_ids)\n",
    "    #mass-merge\n",
    "    #(photos now have same order as attributes)\n",
    "    df = pd.merge(df_attrs,photo_ids,on=('person','imagenum'))\n",
    "\n",
    "    assert len(df)==len(df_attrs),\"lost some data when merging dataframes\"\n",
    "\n",
    "    # print(df.shape)\n",
    "    #image preprocessing\n",
    "    all_photos =df['photo_path'].apply(skimage.io.imread)\\\n",
    "                                .apply(lambda img:img[dy:-dy,dx:-dx])\\\n",
    "                                .apply(lambda img: resize(img,[dimx,dimy]))\n",
    "\n",
    "    all_photos = np.stack(all_photos.values)#.astype('uint8')\n",
    "    all_attrs = df.drop([\"photo_path\",\"person\",\"imagenum\"],axis=1)\n",
    "    \n",
    "    return all_photos, all_attrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "W3KhlblLYY7P"
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "invalid literal for int() with base 10: '0004-checkpoint'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [4]\u001b[0m, in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# The following line fetches you two datasets: images, usable for autoencoder training and attributes.\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# Those attributes will be required for the final part of the assignment (applying smiles), so please keep them in mind\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m data, attrs \u001b[38;5;241m=\u001b[39m \u001b[43mfetch_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [2]\u001b[0m, in \u001b[0;36mfetch_dataset\u001b[0;34m(attrs_name, images_name, dx, dy, dimx, dimy)\u001b[0m\n\u001b[1;32m     32\u001b[0m             photo_id \u001b[38;5;241m=\u001b[39m fname[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m4\u001b[39m]\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39msplit()\n\u001b[1;32m     33\u001b[0m             person_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(photo_id[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n\u001b[0;32m---> 34\u001b[0m             photo_number \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mphoto_id\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     35\u001b[0m             photo_ids\u001b[38;5;241m.\u001b[39mappend({\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mperson\u001b[39m\u001b[38;5;124m'\u001b[39m:person_id,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimagenum\u001b[39m\u001b[38;5;124m'\u001b[39m:photo_number,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mphoto_path\u001b[39m\u001b[38;5;124m'\u001b[39m:fpath})\n\u001b[1;32m     37\u001b[0m photo_ids \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(photo_ids)\n",
      "\u001b[0;31mValueError\u001b[0m: invalid literal for int() with base 10: '0004-checkpoint'"
     ]
    }
   ],
   "source": [
    "# The following line fetches you two datasets: images, usable for autoencoder training and attributes.\n",
    "# Those attributes will be required for the final part of the assignment (applying smiles), so please keep them in mind\n",
    "\n",
    "\n",
    "data, attrs = fetch_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8MSzXXGoYY7X"
   },
   "source": [
    "\n",
    "Разбейте выборку картинок на train и val, выведите несколько картинок в output, чтобы посмотреть, как они выглядят, и приведите картинки к тензорам pytorch, чтобы можно было скормить их сети:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dFc8lTm_YY7Y",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "<тут Ваш код>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z9CC-DUhYY7i"
   },
   "source": [
    "## 1.2. Архитектура модели (1.5 балла)\n",
    "В этом разделе мы напишем и обучем обычный автоэнкодер.\n",
    "\n",
    "\n",
    "\n",
    "<img src=\"https://www.notion.so/image/https%3A%2F%2Fs3-us-west-2.amazonaws.com%2Fsecure.notion-static.com%2F4b8adf79-8e6a-4b7d-9061-8617a00edbb1%2F__2021-04-30__14.53.33.png?table=block&id=56f187b4-279f-4208-b1ed-4bda5f91bfc0&width=2880&userId=3b1b5e32-1cfb-4b0f-8705-5a524a8f56e3&cache=v2\" alt=\"Autoencoder\">\n",
    "\n",
    "\n",
    "^ напомню, что автоэнкодер выглядит вот так"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "csrNCYh-YY7j"
   },
   "outputs": [],
   "source": [
    "dim_code = <your code here> # выберите размер латентного вектора"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fjr-N8AWee-k"
   },
   "source": [
    "Реализуем autoencoder. Архитектуру (conv, fully-connected, ReLu, etc) можете выбирать сами. Экспериментируйте!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8SjHNX-rYY7k"
   },
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "\n",
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        <определите архитектуры encoder и decoder>\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        <реализуйте forward проход автоэнкодера\n",
    "        в качестве ваозвращаемых переменных -- латентное представление картинки (latent_code) \n",
    "        и полученная реконструкция изображения (reconstruction)>\n",
    "        \n",
    "        return reconstruction, latent_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "73lg3bI2YY7m"
   },
   "outputs": [],
   "source": [
    "criterion = <loss>\n",
    "\n",
    "autoencoder = Autoencoder()\n",
    "\n",
    "optimizer = <Ваш любимый оптимизатор>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GpntmZCe5L6i"
   },
   "source": [
    "## 1.3 Обучение (2 балла)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Bdxg_3WJYY7o"
   },
   "source": [
    "Осталось написать код обучения автоэнкодера. При этом было бы неплохо в процессе иногда смотреть, как автоэнкодер реконструирует изображения на данном этапе обучения. Наример, после каждой эпохи (прогона train выборки через автоэекодер) можно смотреть, какие реконструкции получились для каких-то изображений val выборки.\n",
    "\n",
    "А, ну еще было бы неплохо выводить графики train и val лоссов в процессе тренировки =)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3H3DOojrYY7o",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "<тут Ваш код тренировки автоэнкодера>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FAztAMA4YY7q"
   },
   "source": [
    "Давайте посмотрим, как наш тренированный автоэекодер кодирует и восстанавливает картинки:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "I1J__yvxYY7r"
   },
   "outputs": [],
   "source": [
    "< тут Ваш код: выведите первые Х картинок и их реконструкций из val выборки на экран>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1OPh9O6UYY7s"
   },
   "source": [
    "Not bad, right? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dFi96giuYY7t"
   },
   "source": [
    "## 1.4. Sampling (2 балла)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AOtUaPNYYY7t"
   },
   "source": [
    "Давайте теперь будем не просто брать картинку, прогонять ее через автоэекодер и получать реконструкцию, а попробуем создать что-то НОВОЕ\n",
    "\n",
    "Давайте возьмем и подсунем декодеру какие-нибудь сгенерированные нами векторы (например, из нормального распределения) и посмотрим на результат реконструкции декодера:\n",
    "\n",
    "__Подсказка:__Е сли вместо лиц у вас выводится непонятно что, попробуйте посмотреть, как выглядят латентные векторы картинок из датасета. Так как в обучении нейронных сетей есть определенная доля рандома, векторы латентного слоя могут быть распределены НЕ как `np.random.randn(25, <latent_space_dim>)`. А чтобы у нас получались лица при запихивании вектора декодеру, вектор должен быть распределен так же, как латентные векторы реальных фоток. Так что в таком случае придется рандом немного подогнать."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8IZykARRYY7u",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# сгенерируем 25 рандомных векторов размера latent_space\n",
    "z = np.random.randn(25, <latent_space_dim>)\n",
    "output = <скормите z декодеру>\n",
    "<выведите тут полученные картинки>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "id": "Ey8dD9s0YY7w",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "## 1.5 Time to make fun! (4 балла)\n",
    "\n",
    "Давайте научимся пририсовывать людям улыбки =)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i1v-8WwuYY7w"
   },
   "source": [
    "<img src=\"https://i.imgur.com/tOE9rDK.png\" alt=\"linear\" width=\"700\" height=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eGE0M2GDYY7x"
   },
   "source": [
    "План такой:\n",
    "\n",
    "1. Нужно выделить \"вектор улыбки\": для этого нужно из выборки изображений найти несколько (~15) людей с улыбками и столько же без.\n",
    "\n",
    "Найти людей с улыбками вам поможет файл с описанием датасета, скачанный вместе с датасетом. В нем указаны имена картинок и присутствубщие атрибуты (улыбки, очки...)\n",
    "\n",
    "2. Вычислить латентный вектор для всех улыбающихся людей (прогнать их через encoder) и то же для всех грустненьких\n",
    "\n",
    "3. Вычислить, собственно, вектор улыбки -- посчитать разность между средним латентным вектором улыбающихся людей и средним латентным вектором грустных людей\n",
    "\n",
    "4. А теперь приделаем улыбку грустному человеку: добавим полученный в пункте 3 вектор к латентному вектору грустного человека и прогоним полученный вектор через decoder. Получим того же человека, но уже не грустненького!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f1oBX9EeYY7x"
   },
   "outputs": [],
   "source": [
    "<ваш код здесь>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bXI6jprOYY7z"
   },
   "source": [
    "Вуаля! Вы восхитительны!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E2UAf0bpYY70"
   },
   "source": [
    "Теперь вы можете пририсовывать людям не только улыбки, но и много чего другого -- закрывать/открывать глаза, пририсовывать очки... в общем, все, на что хватит фантазии и на что есть атрибуты в `all_attrs`:)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QQnEGmknYY71"
   },
   "source": [
    "# Часть 2: Variational Autoencoder (10 баллов) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bWQNRjJq2uTz"
   },
   "source": [
    "Займемся обучением вариационных автоэнкодеров — проапгрейженной версии AE. Обучать будем на датасете MNIST, содержащем написанные от руки цифры от 0 до 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qBXXr9njByYC"
   },
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "# MNIST Dataset\n",
    "train_dataset = datasets.MNIST(root='./mnist_data/', train=True, transform=transforms.ToTensor(), download=True)\n",
    "test_dataset = datasets.MNIST(root='./mnist_data/', train=False, transform=transforms.ToTensor(), download=False)\n",
    "\n",
    "# Data Loader (Input Pipeline)\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2rHphW5l8Wgi"
   },
   "source": [
    "## 2.1 Архитектура модели и обучение (2 балла)\n",
    "\n",
    "Реализуем VAE. Архитектуру (conv, fully-connected, ReLu, etc) можете выбирать сами. Рекомендуем пользоваться более сложными моделями, чем та, что была на семинаре:) Экспериментируйте!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IoNVT5tYYY74"
   },
   "outputs": [],
   "source": [
    "class VAE(nn.Module):\n",
    "    def __init__(self):\n",
    "        <определите архитектуры encoder и decoder\n",
    "        помните, у encoder должны быть два \"хвоста\", \n",
    "        т.е. encoder должен кодировать картинку в 2 переменные -- mu и logsigma>\n",
    "\n",
    "    def encode(self, x):\n",
    "        <реализуйте forward проход энкодера\n",
    "        в качестве ваозвращаемых переменных -- mu и logsigma>\n",
    "        \n",
    "        return mu, logsigma\n",
    "    \n",
    "    def gaussian_sampler(self, mu, logsigma):\n",
    "        if self.training:\n",
    "            <засемплируйте латентный вектор из нормального распределения с параметрами mu и sigma>\n",
    "        else:\n",
    "            # на инференсе возвращаем не случайный вектор из нормального распределения, а центральный -- mu. \n",
    "            # на инференсе выход автоэнкодера должен быть детерминирован.\n",
    "            return mu\n",
    "    \n",
    "    def decode(self, z):\n",
    "        <реализуйте forward проход декодера\n",
    "        в качестве возвращаемой переменной -- reconstruction>\n",
    "        \n",
    "        return reconstruction\n",
    "\n",
    "    def forward(self, x):\n",
    "        <используя encode и decode, реализуйте forward проход автоэнкодера\n",
    "        в качестве ваозвращаемых переменных -- mu, logsigma и reconstruction>\n",
    "        return mu, logsigma, reconstruction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hAB77d-PYY76"
   },
   "source": [
    "Определим лосс и его компоненты для VAE:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UxJrkXGQo5bp"
   },
   "source": [
    "Надеюсь, вы уже прочитали материал в towardsdatascience (или еще где-то) про VAE и знаете, что лосс у VAE состоит из двух частей: KL и log-likelihood.\n",
    "\n",
    "Общий лосс будет выглядеть так:\n",
    "\n",
    "$$\\mathcal{L} = -D_{KL}(q_{\\phi}(z|x)||p(z)) + \\log p_{\\theta}(x|z)$$\n",
    "\n",
    "Формула для KL-дивергенции:\n",
    "\n",
    "$$D_{KL} = -\\frac{1}{2}\\sum_{i=1}^{dimZ}(1+log(\\sigma_i^2)-\\mu_i^2-\\sigma_i^2)$$\n",
    "\n",
    "В качестве log-likelihood возьмем привычную нам кросс-энтропию."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ac5ey7uIYY77"
   },
   "outputs": [],
   "source": [
    "def KL_divergence(mu, logsigma):\n",
    "    \"\"\"\n",
    "    часть функции потерь, которая отвечает за \"близость\" латентных представлений разных людей\n",
    "    \"\"\"\n",
    "    loss = <напишите код для KL-дивергенции, пользуясь формулой выше>\n",
    "    return \n",
    "\n",
    "def log_likelihood(x, reconstruction):\n",
    "    \"\"\"\n",
    "    часть функции потерь, которая отвечает за качество реконструкции (как mse в обычном autoencoder)\n",
    "    \"\"\"\n",
    "    loss = <binary cross-entropy>\n",
    "    return loss(reconstruction, x)\n",
    "\n",
    "def loss_vae(x, mu, logsigma, reconstruction):\n",
    "    return <соедините тут две компоненты лосса. Mind the sign!>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZPJQu70eYY79"
   },
   "source": [
    "И обучим модель:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dtCjfqXdYY79"
   },
   "outputs": [],
   "source": [
    "criterion = loss_vae\n",
    "\n",
    "autoencoder = VAE()\n",
    "\n",
    "optimizer = <Ваш любимый оптимизатор>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rY1khca6YY7_",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "<обучите модель на датасете MNIST>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SkxW_8fkYY8B",
    "scrolled": true
   },
   "source": [
    "Давайте посмотрим, как наш тренированный VAE кодирует и восстанавливает картинки:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4Jd3BWM_YY8C"
   },
   "outputs": [],
   "source": [
    "< тут Ваш код: выведите первые Х картинок и их реконструкций из val выборки на экран>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PQXYIXjoYY8F"
   },
   "source": [
    "Давайте попробуем проделать для VAE то же, что и с обычным автоэнкодером -- подсунуть decoder'у из VAE случайные векторы из нормального распределения и посмотреть, какие картинки получаются:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bOhhH-osYY8G"
   },
   "outputs": [],
   "source": [
    "# вспомните про замечание из этого же пункта обычного AE про распределение латентных переменных\n",
    "z = np.array([np.random.normal(0, 1, 100) for i in range(10)])\n",
    "output = <скормите z декодеру>\n",
    "<выведите тут полученные картинки>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Nzt-ENxCr6ul"
   },
   "source": [
    "## 2.2. Latent Representation (2 балла)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uIWy670xr-Uv"
   },
   "source": [
    "Давайте посмотрим, как латентные векторы картинок лиц выглядят в пространстве.\n",
    "Ваша задача -- изобразить латентные векторы картинок точками в двумерном просторанстве. \n",
    "\n",
    "Это позволит оценить, насколько плотно распределены латентные векторы изображений цифр в пространстве. \n",
    "\n",
    "Плюс давайте сделаем такую вещь: покрасим точки, которые соответствуют картинкам каждой цифры, в свой отдельный цвет\n",
    "\n",
    "Подсказка: красить -- это просто =) У plt.scatter есть параметр c (color), см. в документации.\n",
    "\n",
    "\n",
    "Итак, план:\n",
    "1. Получить латентные представления картинок тестового датасета\n",
    "2. С помощтю `TSNE` (есть в `sklearn`) сжать эти представления до размерности 2 (чтобы можно было их визуализировать точками в пространстве)\n",
    "3. Визуализировать полученные двумерные представления с помощью `matplotlib.scatter`, покрасить разными цветами точки, соответствующие картинкам разных цифр."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Bk94C6mCsx9c"
   },
   "outputs": [],
   "source": [
    "<ваш код получения латентных представлений, применения TSNE и визуализации>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ifxhsvPss5h_"
   },
   "source": [
    "Что вы думаете о виде латентного представления?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ESPBHrL3YY8H"
   },
   "source": [
    "__Congrats v2.0!__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yIYuKFwijN2U"
   },
   "source": [
    "## 2.3. Conditional VAE (6 баллов)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c5l8Bu1RPjUx"
   },
   "source": [
    "Мы уже научились обучать обычный AE на датасете картинок и получать новые картинки, используя генерацию шума и декодер. \n",
    "Давайте теперь допустим, что мы обучили AE на датасете MNIST и теперь хотим генерировать новые картинки с числами с помощью декодера (как выше мы генерили рандомные лица). \n",
    "И вот нам понадобилось сгенерировать цифру 8, и мы подставляем разные варианты шума, но восьмерка никак не генерится:(\n",
    "\n",
    "Хотелось бы добавить к нашему AE функцию \"выдай мне рандомное число из вот этого вот класса\", где классов десять (цифры от 0 до 9 образуют десять классов).  Conditional AE — так называется вид автоэнкодера, который предоставляет такую возможность. Ну, название \"conditional\" уже говорит само за себя.\n",
    "\n",
    "И в этой части задания мы научимся такие обучать."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0j8zNIwKPY-6"
   },
   "source": [
    "### Архитектура\n",
    "\n",
    "На картинке ниже представлена архитектура простого Conditional VAE.\n",
    "\n",
    "По сути, единственное отличие от обычного -- это то, что мы вместе с картинкой в первом слое энкодера и декодера передаем еще информацию о классе картинки. \n",
    "\n",
    "То есть, в первый (входной) слой энкодера подается конкатенация картинки и информации о классе (например, вектора из девяти нулей и одной единицы). В первый слой декодера подается конкатенация латентного вектора и информации о классе."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y6YloFEAPeM4"
   },
   "source": [
    "\n",
    "![alt text](https://sun9-63.userapi.com/impg/Mh1akf7mfpNoprrSWsPOouazSmTPMazYYF49Tw/djoHNw_9KVA.jpg?size=1175x642&quality=96&sign=e88baec5f9bb91c8443fba31dcf0a4df&type=album)\n",
    "\n",
    "![alt text](https://sun9-73.userapi.com/impg/UDuloLNKhzTBYAKewgxke5-YPsAKyGOqA-qCRg/MnyCavJidxM.jpg?size=1229x651&quality=96&sign=f2d21bfacc1c5755b76868dc4cfef39c&type=album)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hxg2tDSfRbLF"
   },
   "source": [
    "На всякий случай: это VAE, то есть, latent у него все еще состоит из mu и sigma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GpFbSXLaPrm1"
   },
   "source": [
    "Таким образом, при генерации новой рандомной картинки мы должны будем передать декодеру сконкатенированные латентный вектор и класс картинки."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cX0zxklMPwI2"
   },
   "source": [
    "P.S. Также можно передавать класс картинки не только в первый слой, но и в каждый слой сети. То есть на каждом слое конкатенировать выход из предыдущего слоя и информацию о классе."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ar701cHOkDKS"
   },
   "outputs": [],
   "source": [
    "class CVAE(nn.Module):\n",
    "    def __init__(self):\n",
    "        <определите архитектуры encoder и decoder\n",
    "        помните, у encoder должны быть два \"хвоста\", \n",
    "        т.е. encoder должен кодировать картинку в 2 переменные -- mu и logsigma>\n",
    "\n",
    "    def encode(self, x, class_num):\n",
    "        <реализуйте forward проход энкодера\n",
    "        в качестве ваозвращаемых переменных -- mu, logsigma и класс картинки>\n",
    "        \n",
    "        return mu, logsigma, class_num\n",
    "    \n",
    "    def gaussian_sampler(self, mu, logsigma):\n",
    "        if self.training:\n",
    "            <засемплируйте латентный вектор из нормального распределения с параметрами mu и sigma>\n",
    "        else:\n",
    "            # на инференсе возвращаем не случайный вектор из нормального распределения, а центральный -- mu. \n",
    "            # на инференсе выход автоэнкодера должен быть детерминирован.\n",
    "            return mu\n",
    "    \n",
    "    def decode(self, z, class_num):\n",
    "        <реализуйте forward проход декодера\n",
    "        в качестве возвращаемой переменной -- reconstruction>\n",
    "        \n",
    "        return reconstruction\n",
    "\n",
    "    def forward(self, x):\n",
    "        <используя encode и decode, реализуйте forward проход автоэнкодера\n",
    "        в качестве ваозвращаемых переменных -- mu, logsigma и reconstruction>\n",
    "        return mu, logsigma, reconstruction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VoMw-IFyP5A2"
   },
   "source": [
    "### Sampling\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qe1zWyZHkLV2"
   },
   "source": [
    "Тут мы будем сэмплировать из CVAE. Это прикольнее, чем сэмплировать из простого AE/VAE: тут можно взять один и тот же латентный вектор и попросить CVAE восстановить из него картинки разных классов!\n",
    "Для MNIST вы можете попросить CVAE восстановить из одного латентного вектора, например, картинки цифры 5 и 7."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A0SQIhvNP9Dr"
   },
   "outputs": [],
   "source": [
    "<тут нужно научиться сэмплировать из декодера цифры определенного класса>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nAWBu8rzQBgQ"
   },
   "source": [
    "Splendid! Вы великолепны!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rt2S77cm3O1v"
   },
   "source": [
    "### Latent Representations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Nt7x8Ek_rHTE"
   },
   "source": [
    "Давайте посмотрим, как выглядит латентное пространство картинок в CVAE и сравним с картинкой для VAE =)\n",
    "\n",
    "Опять же, нужно покрасить точки в разные цвета в зависимости от класса."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LSCYK7sH3KEc"
   },
   "outputs": [],
   "source": [
    "<ваш код получения латентных представлений, применения TSNE и визуализации>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ET8IELWu3Z2c"
   },
   "source": [
    "Что вы думаете насчет этой картинки? Отличается от картинки для VAE?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SWkqHjvTCD_8"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KN3D_k5W_WZz"
   },
   "source": [
    "# BONUS 1: Denoising\n",
    "\n",
    "## Внимание! За бонусы доп. баллы не ставятся, но вы можете сделать их для себя."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "12a1jkpkCsIU"
   },
   "source": [
    "У автоэнкодеров, кроме сжатия и генерации изображений, есть другие практические применения. Про одно из них эта бонусная часть задания."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v8EN-8jlCtmd"
   },
   "source": [
    "Автоэнкодеры могут быть использованы для избавления от шума на фотографиях (denoising). Для этого их нужно обучить специальным образом: input картинка будет зашумленной, а выдавать автоэнкодер должен будет картинку без шума. \n",
    "То есть, loss-функция AE останется той же (MSE между реальной картинкой и выданной), а на вход автоэнкодеру будет подаваться зашумленная картинка."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j1OJg6jhlaZl"
   },
   "source": [
    "<a href=\"https://ibb.co/YbRJ1nZ\"><img src=\"https://i.ibb.co/0QD164t/Screen-Shot-2020-06-04-at-4-49-50-PM.png\" alt=\"Screen-Shot-2020-06-04-at-4-49-50-PM\" border=\"0\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ysI0BCuRDbvm"
   },
   "source": [
    "Для этого нужно взять ваш любимый датасет (датасет лиц из первой части этого задания или любой другой) и сделать копию этого датасета с шумом. \n",
    "\n",
    "В питоне шум можно добавить так:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "X5e746iVDgSm"
   },
   "outputs": [],
   "source": [
    "noise_factor = 0.5\n",
    "X_noisy = X + noise_factor * np.random.normal(loc=0.0, scale=1.0, size=X.shape) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9fSPkXMtDpd5"
   },
   "outputs": [],
   "source": [
    "<тут ваш код обучения автоэнкодера на зашумленных картинках. Не забудтье разбить на train/test!>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "B03NQ_sKDvg2"
   },
   "outputs": [],
   "source": [
    "<тут проверка, как AE убирает щум с тестовых картинок. Надеюсь, все получилось =)>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-NDiCPYLm2bY"
   },
   "source": [
    "# BONUS 2: Image Retrieval\n",
    "\n",
    "## Внимание! За бонусы доп. баллы не ставятся, но вы можете сделать их для себя."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xao_27WMm7AL"
   },
   "source": [
    "Давайте представим, что весь наш тренировочный датасет -- это большая база данных людей. И вот мы получили картинку лица какого-то человека с уличной камеры наблюдения (у нас это картинка из тестового датасета) и хотим понять, что это за человек. Что нам делать? Правильно -- берем наш VAE, кодируем картинку в латентное представление и ищем среди латентныз представлений лиц нашей базы самые ближайшие!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y__bdS23ndeY"
   },
   "source": [
    "План:\n",
    "\n",
    "1. Получаем латентные представления всех лиц тренировочного датасета\n",
    "2. Обучаем на них LSHForest `(sklearn.neighbors.LSHForest)`, например, с `n_estimators=50`\n",
    "3. Берем картинку из тестового датасета, с помощью VAE получаем ее латентный вектор\n",
    "4. Ищем с помощью обученного LSHForest ближайшие из латентных представлений тренировочной базы\n",
    "5. Находим лица тренировочного датасета, которым соответствуют ближайшие латентные представления, визуализируем!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IksC2ucIoND-"
   },
   "source": [
    "Немного кода вам в помощь: (feel free to delete everything and write your own)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hK0YpLMRoEa0"
   },
   "outputs": [],
   "source": [
    "codes = <поучите латентные представления картинок из трейна>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KisDrgZdoWdt"
   },
   "outputs": [],
   "source": [
    "# обучаем LSHForest\n",
    "from sklearn.neighbors import LSHForest\n",
    "lshf = LSHForest(n_estimators=50).fit(codes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Y_S5zPb5obam"
   },
   "outputs": [],
   "source": [
    "def get_similar(image, n_neighbors=5):\n",
    "  # функция, которая берет тестовый image и с помощью метода kneighbours у LSHForest ищет ближайшие векторы\n",
    "  # прогоняет векторы через декодер и получает картинки ближайших людей\n",
    "\n",
    "  code = <получение латентного представления image>\n",
    "    \n",
    "  (distances,),(idx,) = lshf.kneighbors(code, n_neighbors=n_neighbors)\n",
    "\n",
    "  return distances, X_train[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "t2kjV5wupLP_"
   },
   "outputs": [],
   "source": [
    "def show_similar(image):\n",
    "\n",
    "  # функция, которая принимает тестовый image, ищет ближайшие к нему и визуализирует результат\n",
    "    \n",
    "    distances,neighbors = get_similar(image,n_neighbors=11)\n",
    "    \n",
    "    plt.figure(figsize=[8,6])\n",
    "    plt.subplot(3,4,1)\n",
    "    plt.imshow(image.cpu().numpy().transpose([1,2,0]))\n",
    "    plt.title(\"Original image\")\n",
    "    \n",
    "    for i in range(11):\n",
    "        plt.subplot(3,4,i+2)\n",
    "        plt.imshow(neighbors[i].cpu().numpy().transpose([1,2,0]))\n",
    "        plt.title(\"Dist=%.3f\"%distances[i])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "w3Ja1UNf_oJq"
   },
   "outputs": [],
   "source": [
    "<тут выведите самые похожие лица к какому-нибудь лицу из тестовой части датасета>"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "dFi96giuYY7t",
    "KN3D_k5W_WZz",
    "-NDiCPYLm2bY"
   ],
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
